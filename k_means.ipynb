{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1 Intro\n",
        "\n",
        "This notebook illustrates how to run a k-means clustering analysis in R. We will use the country risk dataset. The dataset contains variables of risk measures for about 120 countries (year unknown). Our goal is to group/cluster these countries based on those risk measures."
      ],
      "metadata": {
        "id": "J_MHIBBQZBZ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2 Load and prepare the data\n",
        "\n",
        "We will\n",
        "\n",
        "1. Load the dataset.\n",
        "2. Perform a simple correlation analysis and decide which risk measures/variables to use for the clustering analysis.\n",
        "3. Standardize the variables to prepare for the clustering  "
      ],
      "metadata": {
        "id": "SKl0icxFbyTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the readxl package (for importing Excel datasets)\n",
        "library(readxl)"
      ],
      "metadata": {
        "id": "ntW6z2oU-VpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the dataset first because read_excel() from readxl package doesn't support reading Excel file from a URL directly\n",
        "data_url <- \"https://github.com/tdmdal/datasets-teaching/raw/main/crisk/country_risk.xlsx\"\n",
        "download.file(url = data_url, destfile = \"country_risk.xlsx\")"
      ],
      "metadata": {
        "id": "kfXXl1ZnAklR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KEKkM-a-LFu"
      },
      "outputs": [],
      "source": [
        "# import the data to a dataframe\n",
        "country_risk <- read_xlsx(path = \"country_risk.xlsx\", sheet = \"raw_kmeans\", skip = 1)\n",
        "head(country_risk)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the structure (str) of the dataframe/tibble\n",
        "str(country_risk)"
      ],
      "metadata": {
        "id": "dwJ2aXYfNQMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find the variable description in the Excel data file (in sheet `data_description`). I also copy them below.\n",
        "\n",
        "| Variable   | Description                                                                      |\n",
        "|------------|----------------------------------------------------------------------------------|\n",
        "| Corruption | Corruption index is on a scale from 0 (high corruption) to 100 (no   corruption) |\n",
        "| Peace      | Peace index is on a scale from 1 (very peaceful) to 5 (not at all   peaceful)   |\n",
        "| Legal      | Legal risk index is on a scale from 0 (high legal risk) to 10 (no legal   risk)  |\n",
        "\n",
        "The four most relevant risk variables/features for clustering are `Corruption`, `Peace`, `Legal`, and `GDP Growth`. We start by perform a correlation analysis on these four variables."
      ],
      "metadata": {
        "id": "P4gEWuBQoGIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# correlation analysis for the four most relevant variables/features\n",
        "cor(country_risk[c(\"Corruption\", \"Peace\", \"Legal\", \"GDP Growth\")])"
      ],
      "metadata": {
        "id": "VNFL13bpIG9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that `Corruption` and `Legal` are highly correlated. As a result, we will just use one of the two for our k-means clustering analysis.\n",
        "\n",
        "We decide to choose `Peace`, `Legal`, and `GDP Growth` for our clustering analysis. We first standardize the three variables/features, i.e., subtract each data point by its column mean and scale the result by the column standard deviation. K-means clustering is a distance based unsupervised learning algorithm so features in large scales can have dominant influence on the clustering result if not scaled properly.\n",
        "\n",
        "We use the `scale()` function in base R for the standardization."
      ],
      "metadata": {
        "id": "Uw0wSZOtc6Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crisk_3col_scaled <- scale(country_risk[c(\"Peace\", \"Legal\", \"GDP Growth\")])\n",
        "str(crisk_3col_scaled)"
      ],
      "metadata": {
        "id": "2zb5ciY8IZSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that `scale()` returns a matrix, with two attributes storing the feature means and standard deviations (SDs). Attribute vector `scale:center` stores the means, and attribute vector `scaled:scale` stores the SDs. As you will see later, we will use those means and SDs to unscale the cluster centroids for the purpose of interpretting (i.e., labeling/naming) the clusters.\n",
        "\n",
        "Let us take a look at scaled data (first 3 rows), and verify that we indeed performed a mean-sd scaling."
      ],
      "metadata": {
        "id": "m0cOu5HxqRBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at first three rows\n",
        "crisk_3col_scaled[1:3, 1:3]"
      ],
      "metadata": {
        "id": "jbuzL-j0MB45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify it's indeed mean and SD scaling.\n",
        "mean_peace <- mean(country_risk$Peace)\n",
        "sd_peace <- sd(country_risk$Peace)\n",
        "peace_scaled <- (country_risk$Peace - mean_peace) / sd_peace\n",
        "\n",
        "peace_scaled[1:3]"
      ],
      "metadata": {
        "id": "739so9elON1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3 K-means clustering"
      ],
      "metadata": {
        "id": "Qo9Ryrc55yvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Determine the $k$\n",
        "\n",
        "The k-means clustering algorithm does not learn the number of clusters, $k$. We need to set the value of $k$ before we run the algorithm. There are many methods to determine $k$. We will use a *heuristic* and *visual* approach called elbow method. Although the elbow method is often used, just note that not all researchers are happy with this method ([Schubert, 2022](https://arxiv.org/abs/2212.12189)).\n",
        "\n",
        "The elbow method plots total within-cluster sum of squares (Total WSS), the measure that the k-mean clustering algorithm minimizes, against the number of clusters. The plotted curve is guaranteed to be decreasing, i.e., total WSS decreases as number of clusters increases. The method picks the number of clusters corresponding to the \"elbow\" of the curve. The \"elbow\" is the point where an additional cluster won't reduce the total WSS too much comparing to the last additional cluster (i.e., the marginal gain of adding a cluster drops sharply at the \"elbow\").\n",
        "\n",
        "Let us walk through the elbow method below."
      ],
      "metadata": {
        "id": "ewtoWEAs57Nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set a random seed so you can reproduce the result\n",
        "set.seed(123)\n",
        "\n",
        "# max number of clusters (k) to try\n",
        "num_cluster <- 8\n",
        "\n",
        "# a vector to hold \"total within sum of squares\" for each number of clusters tried\n",
        "twss <- rep(0, times = 8)\n",
        "\n",
        "# try 1 to num_cluster possible clusters\n",
        "for (i in 1:num_cluster) {\n",
        "  # fit the k-means model\n",
        "  km_fit <- kmeans(crisk_3col_scaled, centers = i, nstart = 10)\n",
        "  # save the total within cluster sum of squares\n",
        "  twss[i] <- km_fit$tot.withinss\n",
        "}\n",
        "\n",
        "k <- 1:num_cluster\n",
        "plot(k, twss, type = \"b\")"
      ],
      "metadata": {
        "id": "72YQZT1uPClH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, total WSS decreases as number of clusters increases. At $k=3$, an additional cluster seems to reduce total WSS much less that from $k=2$ to $3$. I will therefore pick $k=3$, i.e., the elbow is the point corresponding to $k=3$. Again, the elbow method is a *heuristic* way of determine $k$. Often time, it's not a clear cut and there is no precise formula to follow."
      ],
      "metadata": {
        "id": "JmkoCwwE-8Fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.2 perform K-mean clustering ($k=3$)\n",
        "\n",
        "Since we decided to set $k=3$, let us re-fit the model at $k=3$."
      ],
      "metadata": {
        "id": "i9Ut8FOuFAW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "km_fit_3 <- kmeans(crisk_3col_scaled, centers = 3, nstart = 10)\n",
        "km_fit_3"
      ],
      "metadata": {
        "id": "VswDbeU0Uw0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The clustering report prints out the number of data points (countries) in each cluster, the cluster means (i.e. cluster centroids), the cluster labels (1, 2, or 3 in our case), and the distance measures WSS.\n",
        "\n",
        "Note that the returned fitted object (the variable `km_fit_3`) is a named-list, where you can retrieve all the stored information. For example, `km_fit_3$cluster` gives all the cluster labels, and `km_fit_3$tot.withinss` gives the total WSS."
      ],
      "metadata": {
        "id": "qgwBwc_0BTTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# structure of km_fit_3: it is a named list\n",
        "str(km_fit_3)"
      ],
      "metadata": {
        "id": "oPgLWetSVFl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new column \"cluster\" in the original data frame to store the cluster label (1, 2, or 3)\n",
        "country_risk[\"cluster\"] <- km_fit_3$cluster\n",
        "country_risk"
      ],
      "metadata": {
        "id": "llygJ4nuVHSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how many countries we have in each of these three clusters."
      ],
      "metadata": {
        "id": "7bUdTQGoDNzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# count countries in each cluster (this is given in the clustering report too)\n",
        "table(country_risk$cluster)"
      ],
      "metadata": {
        "id": "39i36ltScYRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Interpret/name the clusters\n",
        "\n",
        "To meaningfully label/name each cluster (e.g., high risk cluster, low risk cluster, etc.), we can take a look at the cluster centroids. Countries belong to a certain cluster/group tend to center round the cluster centroid."
      ],
      "metadata": {
        "id": "RZcFNHI8DU8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve cluster centroids\n",
        "centroid <- km_fit_3$center\n",
        "centroid"
      ],
      "metadata": {
        "id": "DxKD-pGEVtIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that these centroids are obtained after variables being scaled. To better interpret them, we can scale the centroids back.\n",
        "\n",
        "We will use the `attributes()` function to retrieve the attributes of the `crisk_3col_scaled` matrix. The `attributes(crisk_3col_scaled)` returns a named list so we can further retrieve the corresponding means and SDs used for the data scaling by referring to `scaled:center` and `scaled:scale` element."
      ],
      "metadata": {
        "id": "RGSuB04nD9Pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the three means and SDs used for scaling\n",
        "str(attributes(crisk_3col_scaled))"
      ],
      "metadata": {
        "id": "3H02wyo5WAqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the means used for scaling\n",
        "scaled_mean <- attributes(crisk_3col_scaled)$\"scaled:center\"\n",
        "print(scaled_mean)"
      ],
      "metadata": {
        "id": "uQQpZrWxXg4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turn the means used for scaling to a matrix with repeated rows\n",
        "scaled_mean_mat <- matrix(scaled_mean, nrow = 3, ncol = 3, byrow = TRUE)\n",
        "scaled_mean_mat"
      ],
      "metadata": {
        "id": "unlPiE-ya1I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turn the SDs used for scaling to a matrix with repeated rows\n",
        "scaled_sd <- attributes(crisk_3col_scaled)$\"scaled:scale\"\n",
        "scaled_sd_mat <- matrix(scaled_sd, nrow = 3, ncol = 3, byrow = TRUE)\n",
        "scaled_sd_mat"
      ],
      "metadata": {
        "id": "xDOtnLpybPOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are ready to scale the centroids back. The inverse operation for the data standardization is simply multiplying SD and then add the mean back."
      ],
      "metadata": {
        "id": "wSESSDXVWtkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scale each centroid back to the original scale\n",
        "# that is, centroid * SD + mean\n",
        "centroid_unscaled <- centroid * scaled_sd_mat + scaled_mean_mat\n",
        "centroid_unscaled"
      ],
      "metadata": {
        "id": "9ssRYKtmXsy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "\n",
        "Can you name the three clusters? Hint: take a look at the data description and understand how `Peace` and `Legal` are scored and their score ranges."
      ],
      "metadata": {
        "id": "2aWw13bzHmKj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TeYtbAN3XxQS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}